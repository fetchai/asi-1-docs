import { ApiEndpointRequestResponse} from "../../../../../components/api-endpoint";
import { Row, Col , Property, Properties} from "../../../../../components/mdx";

import { Callout } from 'nextra/components'



# Introduction 

This API Reference describes the RESTful and streaming interfaces of the platform.

## Open AI Compatability

Where possible we have made every effort to conform to the OpenAI API specification. This largely means that users can simply plug the new API

## Authorization

API keys can be created from your account when you log into ASI1. Authorization is done by adding the following header into your requests
`
Authorization: Bearer <api token>
`

<Callout type="info" emoji="ℹ️">
 Remember your API key is a secret, do not give it to anyone. If you need to remove access for a particular key simply log into your account and delete is from your profile
</Callout>

## Request and response library

### Create chat completion

<ApiEndpointRequestResponse
  apiUrl="https://api.asi1.ai"
  method="POST"
  path="/v1/chat/completions"
  description="Creates a model response for the given chat conversation."
  samplePayload={
 {
    "model": "asi1-mini",
    "messages": [
        {
            "role": "user", "content": "Hello"
        }
    ],
    "temperature":0.7,
    "stream": false,
    "max_tokens": 1024
}
  }

  properties={[
    {
      name: "model",
      type: "string",
      required : true,
      description:
        "ID of the model to use",
    },
     {
      name: "messages",
      type: "object[]",
      required : true,
      description:
        "A list of messages comprising the conversation so far.",
      nestedChildren : [
      {
        name: "role",
        type: "string",
        required: true,
        description: "The role of the author of this message."
      },
      {
        name: "content",
        type: "string",
        required: true,
        description: "The contents of the message."
      }
    ]
    },
     {
      name: "temperature",
      type: "double",
      required : true,
      description:
        "What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic",
    },
    {
      name: "stream",
      type: "boolean",
      required : true,
      description:
        "If set, partial message deltas will be sent. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message",
    },
      {
      name: "max_tokens",
      type: "int64",
      required : true,
      description:
        "The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API.",
    },
    ]}
 
  responses={{
    "model": "asi1-mini",
    "id": "id_kKg27rnGyfH4NknTL",
    "executable_data": [],
    "conversation_id": null,
    "thought": [
        "The user has initiated a simple greeting. This is a standard conversational start, and my response should be polite and professional, introducing myself as ASI1-Mini while highlighting my unique capabilities as an agentic, decentralised-focused model."
    ],
    "tool_thought": [],
    "choices": [
        {
            "index": 0,
            "finish_reason": "stop",
            "message": {
                "role": "assistant",
                "content": "Hello! I'm ASI1-Mini, an advanced agentic and decentralised-focused assistant powered by fetch.ai Inc. I'm designed to support complex workflows with executable expertise. How can I assist you today?"
            }
        }
    ],
    "usage": {
        "prompt_tokens": 39,
        "completion_tokens": 100,
        "total_tokens": 139
    }
}}
  responseDescription="Returns a response with fields like model, id, and conversation_id for identification, thought explaining reasoning, choices containing the generated message, and usage tracking token consumption."
/>